# deloris_ai/vision.py
# [MODULE: VISION v2.7 - LATEST STABLE]
# Fix l·ªói 'PhiForCausalLM object has no attribute generate':
# C·∫≠p nh·∫≠t l√™n phi√™n b·∫£n code m·ªõi nh·∫•t, b·ªè revision c≈©.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import threading
import logging

# T·∫Øt log r√°c
logging.getLogger("transformers").setLevel(logging.ERROR)

class VisionSystem:
    def __init__(self):
        self.model_id = "vikhyatk/moondream2"
        self.model = None
        self.tokenizer = None
        self.is_ready = False
        # X√°c ƒë·ªãnh thi·∫øt b·ªã
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # T·∫£i model trong lu·ªìng ri√™ng
        threading.Thread(target=self._load_model, daemon=True).start()

    def _load_model(self):
        print(f"üëÅÔ∏è [VISION] ƒêang c·∫≠p nh·∫≠t th·ªã gi√°c (Target: {self.device.upper()})...")
        try:
            # [FIX FINAL] B·ªè tham s·ªë 'revision' ƒë·ªÉ l·∫•y code m·ªõi nh·∫•t t·ª´ HuggingFace
            # Gi·ªØ nguy√™n low_cpu_mem_usage=False ƒë·ªÉ tr√°nh l·ªói Meta Tensor
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id, 
                trust_remote_code=True,
                low_cpu_mem_usage=False, # B·∫Øt bu·ªôc False ƒë·ªÉ tr√°nh l·ªói copy
                device_map=None 
            ).to(self.device)
            
            self.model.eval() 
            
            # T·∫£i Tokenizer m·ªõi nh·∫•t
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
            
            self.is_ready = True
            print(f"üëÅÔ∏è [VISION] ƒê√£ m·ªü m·∫Øt th√†nh c√¥ng (v2.7 Latest).")
            
        except Exception as e:
            print(f"‚ö†Ô∏è [VISION ERROR] L·ªói kh·ªüi t·∫°o: {e}")
            # Fallback v·ªÅ CPU n·∫øu GPU g√¢y l·ªói
            if self.device == "cuda":
                print("   -> ƒêang th·ª≠ l·∫°i v·ªõi CPU...")
                self.device = "cpu"
                try:
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_id,
                        trust_remote_code=True,
                        low_cpu_mem_usage=False
                    ).to("cpu")
                    self.model.eval()
                    self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
                    self.is_ready = True
                    print("üëÅÔ∏è [VISION] ƒê√£ m·ªü m·∫Øt (CPU Mode).")
                except Exception as e2:
                    print(f"   -> V·∫´n th·∫•t b·∫°i: {e2}")

    def analyze_image(self, image_path, prompt="Describe this image."):
        """
        Nh√¨n ·∫£nh v√† tr·∫£ v·ªÅ m√¥ t·∫£ vƒÉn b·∫£n.
        """
        if not self.is_ready:
            return "M·∫Øt em ƒëang c·∫≠p nh·∫≠t, ƒë·ª£i m·ªôt ch√∫t..."
            
        try:
            image = Image.open(image_path)
            
            # M√£ h√≥a ·∫£nh b·∫±ng model
            enc_image = self.model.encode_image(image)
            
            # T·∫°o m√¥ t·∫£
            description = self.model.answer_question(enc_image, prompt, self.tokenizer)
            
            return description
        except Exception as e:
            print(f"‚ö†Ô∏è [VISION ERROR] Nh√¨n nh·∫ßm: {e}")
            return "Em th·∫•y h∆°i m·ªù, kh√¥ng nh√¨n r√µ l·∫Øm."

    def detect_emotion_in_image(self, image_path):
        return self.analyze_image(image_path, "What is the emotional atmosphere of this image?")

# Instance to√†n c·ª•c
deloris_eye = VisionSystem()